{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "import seaborn as sns; sns.set()\n",
    "import lightgbm as lgb\n",
    "from sklearn import preprocessing, metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import gc\n",
    "import os\n",
    "import time\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.stats import poisson\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from math import ceil\n",
    "%env JOBLIB_TEMP_FOLDER=/tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Useful links \n",
    "\n",
    "* https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/163216\n",
    "* https://www.kaggle.com/c/m5-forecasting-accuracy/discussion/174371"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Memory reduction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to reduce the memory usage for all the given data sets\n",
    "\n",
    "def mem_usage_reduction(df):\n",
    "\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Mem. usage decreased to {:5.2f} Mb from {:5.2f} ({:.1f}% reduction)'.format(end_mem,start_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Melting and Merging the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changing_data(calendar,sell_prices,sales_train, sample_submission,nrows):\n",
    "        \n",
    "    # We are melting the sales_train data based on days('d_x')\n",
    "    sales_train = pd.melt(sales_train, id_vars = ['id','item_id','dept_id','cat_id','store_id','state_id'],\n",
    "                            var_name = 'd',value_name = 'unit_sales')\n",
    "    \n",
    "    \n",
    "    # Creating the final evaluation data\n",
    "    evaluation_rows = [row for row in sample_submission['id'] if 'evaluation' in row]\n",
    "    evaluation_data = sample_submission[sample_submission['id'].isin(evaluation_rows)]\n",
    "  \n",
    "    # Changing the column names to the respective daywise representations\n",
    "    id2 = ['id']\n",
    "    day_eval_columns = [f'd_{row}' for row in range(1942,1970)]\n",
    "    id2.extend(day_eval_columns)\n",
    "    evaluation_data.columns = id2\n",
    "    \n",
    "    \n",
    "    # Product id's table\n",
    "    product_ids = sales_train[['id','item_id','dept_id','cat_id','store_id','state_id']].drop_duplicates()\n",
    "\n",
    "    \n",
    "    # merging evaluation data with product_ids columns\n",
    "    evaluation_data = evaluation_data.merge(product_ids, how ='left', on='id')\n",
    "    \n",
    "    \n",
    "    # Melting the evaluation data\n",
    "       \n",
    "    evaluation_data = pd.melt(evaluation_data, id_vars = ['id', 'item_id', 'dept_id','cat_id', 'store_id','state_id'],\n",
    "                          var_name = 'd', value_name = 'unit_sales')\n",
    "    \n",
    "    # Adding a columns that separates train and evaluation\n",
    "    sales_train['data'] = 'train'\n",
    "    evaluation_data['data'] = 'evaluation'\n",
    "    \n",
    "    \n",
    "    # Concatenating train and evaluation\n",
    "    data = pd.concat([sales_train,evaluation_data],axis=0)\n",
    "\n",
    "    del sales_train, evaluation_data\n",
    "    \n",
    "    \n",
    "    data = mem_usage_reduction(data)\n",
    "    \n",
    "    # Taking only a subset of data for fast training\n",
    "    \n",
    "    data = data.iloc[nrows:]\n",
    "    \n",
    "    # Adding sell_prices data to the train data along with new columns\n",
    "    \n",
    "    sell_prices['price_max']     =sell_prices.groupby(['store_id','item_id'])['sell_price'].transform('max')\n",
    "    sell_prices['price_min']     =sell_prices.groupby(['store_id','item_id'])['sell_price'].transform('min')\n",
    "    sell_prices['price_std']     =sell_prices.groupby(['store_id','item_id'])['sell_price'].transform('std')\n",
    "    sell_prices['price_mean']    =sell_prices.groupby(['store_id','item_id'])['sell_price'].transform('mean')\n",
    "    sell_prices['price_max']     =sell_prices['sell_price']/sell_prices['price_max']\n",
    "    sell_prices['price_nunique'] =sell_prices.groupby(['store_id','item_id'])['sell_price'].transform('nunique')\n",
    "    sell_prices['item_nunique']  =sell_prices.groupby(['store_id','sell_price'])['item_id'].transform('nunique')\n",
    "\n",
    "    calendar_prices = calendar[['wm_yr_wk','month','year']]\n",
    "    calendar_prices = calendar_prices.drop_duplicates(subset=['wm_yr_wk'])\n",
    "    sell_prices = sell_prices.merge(calendar_prices[['wm_yr_wk','month','year']],on=['wm_yr_wk'],how='left')\n",
    "\n",
    "    del calendar_prices\n",
    "\n",
    "    #sell_prices['price_momentum']= sell_prices['sell_price']/sell_prices.groupby(['store_id','item_id'])['sell_price'].transform(lambda x:x.shift(1))\n",
    "    #sell_prices['price_momentum_m']= sell_prices['sell_price']/sell_prices.groupby(['store_id','item_id','month'])['sell_price'].transform('mean')\n",
    "    #sell_prices['price_momentum_y']= sell_prices['sell_price']/sell_prices.groupby(['store_id','item_id','year'])['sell_price'].transform('mean')\n",
    "\n",
    "    #sell_prices[['store_id','item_id','release']] = sell_prices.groupby(['store_id','item_id'])['wm_yr_wk'].agg(['min']).reset_index()\n",
    "    #release_df.columns = ['store_id','item_id','release']\n",
    "    #d = data[['store_id','item_id']]\n",
    "    #d = d.merge(sell_prices[['store_id','item_id','release']],on=['store_id','item_id'],how='left')\n",
    "    #new_columns = [col for col in list(d) if col not in ['store_id','item_id']]\n",
    "    #data = pd.concat([data,d[new_columns]])\n",
    "    #del d,new_columns\n",
    "    \n",
    "    # Dropping few features from calender \n",
    "    calendar.drop([\"weekday\",\"wday\",\"month\",\"year\"], inplace = True, axis =1)\n",
    "\n",
    "    data = pd.merge(data, calendar, how = 'left',on = ['d'])\n",
    "    data.drop(['d'], inplace = True, axis = 1)\n",
    "    \n",
    "    data = data.merge(sell_prices, on=['store_id', 'item_id','wm_yr_wk'], how = 'left')\n",
    "    \n",
    "    del calendar,sell_prices\n",
    "    \n",
    "    gc.collect()\n",
    "    return data\n",
    "    \n",
    "    \n",
    "#4390560   for 5 years data\n",
    "#15519410  for 4 years data\n",
    "#26648260  for 3 years data\n",
    "#37777110  for 2 years data\n",
    "#48905960  for 1 years data\n",
    "#60034810 total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vectorization - Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforming_data(data):\n",
    "    nan_features = ['event_name_1','event_type_1','event_name_2','event_type_2']\n",
    "    for i in nan_features:\n",
    "        data[i].fillna('no_event',inplace=True)\n",
    "        \n",
    "    encoder = preprocessing.LabelEncoder()\n",
    "    data['encoded_id']=encoder.fit_transform(data['id'])\n",
    "    \n",
    "    cat_feat = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', \n",
    "                'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n",
    "    \n",
    "    for fe in cat_feat:\n",
    "        data[fe]=encoder.fit_transform(data[fe])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "       \n",
    "    # Adding shift and rolling_mean features\n",
    "    lag_list = [7,8,9,14,15,16,21,22,23,28,29,30]\n",
    "    rolling_list = [7,14,30]\n",
    "\n",
    "    for val in lag_list:\n",
    "        df[f\"lag_d_{val}\"]=df.groupby(['id'])['unit_sales'].transform(lambda x:x.shift(val))\n",
    "    #print('done1')\n",
    "    for val in rolling_list:\n",
    "        df[f\"r_std_d{val}\"] = df.groupby([\"id\"])[\"unit_sales\"].transform(lambda x:x.shift(28).rolling(val).std())\n",
    "    #print('done2')\n",
    "    for val in rolling_list:\n",
    "        df[f\"r_mean_d{val}\"] = df.groupby(['id'])['unit_sales'].transform(lambda x:x.shift(28).rolling(val).mean())\n",
    "    #print('done3')\n",
    "      \n",
    "\n",
    "    # time features\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    \n",
    "    df['tm_d']=df['date'].dt.day.astype(np.int8)\n",
    "    df['tm_w']=df['date'].dt.week.astype(np.int8)\n",
    "    df['tm_m']=df['date'].dt.month.astype(np.int8)\n",
    "    df['tm_y']=df['date'].dt.year\n",
    "    df['tm_y']=(df['tm_y']-df['tm_y'].min()).astype(np.int8)\n",
    "    df['tm_wm']=df['tm_d'].apply(lambda x: ceil(x/7)).astype(np.int8)\n",
    "    \n",
    "    df['tm_dw']=df['date'].dt.dayofweek.astype(np.int8)\n",
    "    df['tm_w_end']=(df['tm_dw']>=5).astype(np.int8)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.Reading the Data and Memory Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "calendar = pd.read_csv(\"../input/m5-forecasting-accuracy/calendar.csv\")\n",
    "calendar['date']=pd.to_datetime(calendar['date'])\n",
    "calendar = mem_usage_reduction(calendar)\n",
    "sales_train = pd.read_csv(\"../input/m5-forecasting-accuracy/sales_train_evaluation.csv\")\n",
    "sales_train = mem_usage_reduction(sales_train)\n",
    "sell_prices = pd.read_csv(\"../input/m5-forecasting-accuracy/sell_prices.csv\")\n",
    "sell_prices = mem_usage_reduction(sell_prices)\n",
    "sample_submission = pd.read_csv('../input/m5-forecasting-accuracy/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "data = changing_data(calendar,sell_prices,sales_train,sample_submission,nrows=26648260)\n",
    "gc.collect()\n",
    "\n",
    "# Memory reduction function\n",
    "data = mem_usage_reduction(data)\n",
    "\n",
    "# Data transformation\n",
    "data = transforming_data(data)\n",
    "gc.collect()\n",
    "\n",
    "# Memory reduction function\n",
    "data = mem_usage_reduction(data)\n",
    "\n",
    "# Addition of new features\n",
    "data = feature_engineering(data)\n",
    "gc.collect()\n",
    "\n",
    "# Memory reduction function\n",
    "data = mem_usage_reduction(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['month','year','data'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Missing value percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nan_values(data):\n",
    "    total = data.isnull().sum().sort_values(ascending = False)\n",
    "    percent = (data.isnull().sum()/data.isnull().count()*100).sort_values(ascending = False)\n",
    "    missing_train_data  = pd.concat([total, percent], axis=1, keys=['Total', 'Missing_Percentage'])\n",
    "    print(missing_train_data.head(29))\n",
    "nan_values(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Saving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reset_index(drop=True,inplace=True)\n",
    "data.to_hdf('data.h5',key='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### 3. Melting and Merging the data\n",
    "\n",
    "* Melting the data based on 'd'(days) - sales_train\n",
    "* Creating evaluation data for 28 days and melted it based on 'd'(days) - evaluation_data\n",
    "* Concatenating the evaluation data to the melted sales_data. (data)\n",
    "* Taken  3 years data starting from 26648260 till the end.(Saving the memory)\n",
    "* Creating new features in sell_prices and merging with calendar.\n",
    "* Merging calendar and sell_prices features with the data.\n",
    "\n",
    "\n",
    "### 4. Vectorization - Label Encoding\n",
    "\n",
    "* Removing nan values and replacing them with a category.\n",
    "* Label Encoding all the categorical features.\n",
    "\n",
    "\n",
    "\n",
    "### 5. Feature Engineering\n",
    "\n",
    "* Adding new features based on unit_sales.\n",
    "* lag features with multiple shifts and rolling features of mean and standard deviation with a 28 days shift.\n",
    "* New time features- day,week,month,year and weekend are formed based on the 'date' feature.\n",
    "\n",
    "\n",
    "\n",
    "### 6.Reading the Data and Memory Reduction\n",
    "* Reading the data of calendar,sales_train,sell_prices and sample_submission from the competition data.\n",
    "* Reducing the memory usage with mem_usage_reduction function.\n",
    "\n",
    "\n",
    "### 8. Saving the data\n",
    "\n",
    "* Saving the final data to hdf file for further usage of modelling.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
